"""Evaluation tests for specification quality.

This module tests the quality of specifications generated by the
managing-specifications skill using real specification files.
"""

import pytest

from .utils import (
    has_required_spec_fields,
    load_spec_as_eval_output,
)

# Note: Pydantic Evals integration removed since we're using real file-based testing
# All tests now work with actual specification files from .quaestor/specs/


def test_spec_quality_with_real_files(good_quality_spec, poor_quality_spec, edge_case_spec):
    """Test specification quality using real specification files.

    This test demonstrates Level 1 integration - evaluating actual spec files
    that could be created by the managing-specifications skill.

    Uses fixture-provided spec files:
    - good_quality_spec: Should score highly (complete, clear, actionable)
    - poor_quality_spec: Should score poorly (vague, incomplete)
    - edge_case_spec: Complex spec with many criteria
    """

    # Load real specification files
    good_spec = load_spec_as_eval_output(good_quality_spec)
    poor_spec = load_spec_as_eval_output(poor_quality_spec)
    edge_spec = load_spec_as_eval_output(edge_case_spec)

    assert good_spec is not None, "Good quality spec should load successfully"
    assert poor_spec is not None, "Poor quality spec should load successfully"
    assert edge_spec is not None, "Edge case spec should load successfully"

    # Test good quality spec
    print("\n" + "=" * 80)
    print("TESTING GOOD QUALITY SPECIFICATION")
    print("=" * 80)

    good_is_valid, good_missing = has_required_spec_fields(good_spec)
    print(f"✓ Required fields: {'PASS' if good_is_valid else 'FAIL'}")
    if not good_is_valid:
        print(f"  Missing: {', '.join(good_missing)}")

    # Test poor quality spec
    print("\n" + "=" * 80)
    print("TESTING POOR QUALITY SPECIFICATION")
    print("=" * 80)

    poor_is_valid, poor_missing = has_required_spec_fields(poor_spec)
    print(f"✓ Required fields: {'FAIL' if not poor_is_valid else 'UNEXPECTED PASS'}")
    if not poor_is_valid:
        print(f"  Missing (expected): {', '.join(poor_missing)}")

    # Test edge case spec
    print("\n" + "=" * 80)
    print("TESTING EDGE CASE SPECIFICATION")
    print("=" * 80)

    edge_is_valid, _ = has_required_spec_fields(edge_spec)
    print(f"✓ Required fields: {'PASS' if edge_is_valid else 'FAIL'}")
    print(f"  Acceptance criteria count: {len(edge_spec.acceptance_criteria)}")

    # Assertions
    assert good_is_valid, "Good quality spec should have all required fields"
    assert not poor_is_valid, "Poor quality spec should be missing fields"
    assert len(good_spec.test_scenarios) > 0, "Good spec should have test scenarios"
    assert len(good_spec.acceptance_criteria) >= 5, "Good spec should have multiple acceptance criteria"
    assert len(poor_spec.description) < len(good_spec.description), "Poor spec should have shorter description"
    assert len(edge_spec.acceptance_criteria) >= 15, "Edge case spec should have many criteria"

    print("\n" + "=" * 80)
    print("✅ REAL FILE EVALUATION COMPLETE")
    print("=" * 80)


def test_spec_quality_comparison(good_quality_spec, poor_quality_spec):
    """Compare good vs poor quality specifications.

    This test demonstrates comparing multiple spec files to establish
    quality baselines and thresholds.
    """
    # Load both specs
    good_spec = load_spec_as_eval_output(good_quality_spec)
    poor_spec = load_spec_as_eval_output(poor_quality_spec)

    # Compare field completeness
    good_valid, _ = has_required_spec_fields(good_spec)
    poor_valid, _ = has_required_spec_fields(poor_spec)

    # Compare content richness
    good_criteria_count = len(good_spec.acceptance_criteria)
    poor_criteria_count = len(poor_spec.acceptance_criteria)

    good_scenarios_count = len(good_spec.test_scenarios)
    poor_scenarios_count = len(poor_spec.test_scenarios)

    good_description_length = len(good_spec.description)
    poor_description_length = len(poor_spec.description)

    print("\n" + "=" * 80)
    print("SPECIFICATION QUALITY COMPARISON")
    print("=" * 80)
    print("\nGood Spec:")
    print(f"  Fields complete: {good_valid}")
    print(f"  Acceptance criteria: {good_criteria_count}")
    print(f"  Test scenarios: {good_scenarios_count}")
    print(f"  Description length: {good_description_length} chars")

    print("\nPoor Spec:")
    print(f"  Fields complete: {poor_valid}")
    print(f"  Acceptance criteria: {poor_criteria_count}")
    print(f"  Test scenarios: {poor_scenarios_count}")
    print(f"  Description length: {poor_description_length} chars")

    # Assertions
    assert good_valid and not poor_valid, "Good spec should be valid, poor spec should not"
    assert good_criteria_count > poor_criteria_count, "Good spec should have more criteria"
    assert good_scenarios_count > poor_scenarios_count, "Good spec should have more scenarios"
    assert good_description_length > poor_description_length, "Good spec should have longer description"

    print("\n✅ Quality comparison test passed")
    print("=" * 80)


if __name__ == "__main__":
    """Run the evaluation directly."""
    pytest.main([__file__, "-v", "-s"])
